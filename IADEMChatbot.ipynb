{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNTUkAvC/CgdMj/3a23nVjz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ceyda125/AIDEM-Chatbot/blob/main/IADEMChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HhaxILJz1MQ",
        "outputId": "37ca2a69-b064-44c2-e6fb-b468c5bbed6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab hücresine\n",
        "!pip install -q sentence-transformers faiss-cpu fastapi uvicorn nest-asyncio pyngrok openai\n"
      ],
      "metadata": {
        "id": "JLXyqK1ywtyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#en son bunu yaptım!\n",
        "\n",
        "import re\n",
        "import json\n",
        "\n",
        "# --- 1. DOSYAYI OKU ---\n",
        "file_path = \"/content/drive/MyDrive/EnerjiChatbot/enerji_rehberi.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    energy_text = f.read()\n",
        "\n",
        "# --- 2. METNİ TEMİZLEYEN VE PARÇALAYAN FONKSİYON ---\n",
        "def chunk_text(text, max_words=120):\n",
        "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
        "\n",
        "    # Satırlara böl\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    chunks = []\n",
        "    current_title = \"\"\n",
        "    current_content = \"\"\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        if line.startswith(\"Konu:\"):\n",
        "            # Önceki konu varsa chunk yap\n",
        "            if current_title:\n",
        "                full_text = current_title + \"\\n\" + current_content\n",
        "                words = full_text.split()\n",
        "                if len(words) <= max_words:\n",
        "                    chunks.append(full_text.strip())\n",
        "                else:\n",
        "                    for i in range(0, len(words), max_words):\n",
        "                        chunks.append(\" \".join(words[i:i+max_words]))\n",
        "            # Yeni konu başlığı\n",
        "            current_title = line\n",
        "            current_content = \"\"\n",
        "        else:\n",
        "            if current_content:\n",
        "                current_content += \" \" + line\n",
        "            else:\n",
        "                current_content = line\n",
        "\n",
        "    # Son konuyu ekle\n",
        "    if current_title:\n",
        "        full_text = current_title + \"\\n\" + current_content\n",
        "        words = full_text.split()\n",
        "        if len(words) <= max_words:\n",
        "            chunks.append(full_text.strip())\n",
        "        else:\n",
        "            for i in range(0, len(words), max_words):\n",
        "                chunks.append(\" \".join(words[i:i+max_words]))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# --- 3. FONKSİYONU ÇALIŞTIR ---\n",
        "chunks = chunk_text(energy_text, max_words=120)\n",
        "\n",
        "# --- 4. JSON olarak kaydet ---\n",
        "json_path = \"/content/drive/MyDrive/EnerjiChatbot/enerji_chunks.json\"\n",
        "json_data = [{\"id\": i, \"text\": c} for i, c in enumerate(chunks)]\n",
        "\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Toplam chunk sayısı: {len(chunks)}\")\n",
        "print(\"İlk 5 chunk:\")\n",
        "for c in chunks[:5]:\n",
        "    print(\"----\")\n",
        "    print(c)\n"
      ],
      "metadata": {
        "id": "mhhIYykz4_jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerekli kütüphaneler\n",
        "import json\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- JSON'dan chunk'ları oku ---\n",
        "json_path = \"/content/drive/MyDrive/EnerjiChatbot/enerji_chunks.json\"\n",
        "\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "texts = [chunk[\"text\"] for chunk in data]\n",
        "print(f\"Toplam chunk sayısı: {len(texts)}\")\n",
        "\n",
        "# --- SentenceTransformer ile embedding oluştur ---\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "# Embeddingleri üret\n",
        "embeddings = model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "# FAISS ile uyumlu olacak şekilde float32 tipine çevir\n",
        "embeddings = np.array(embeddings).astype(\"float32\")\n",
        "\n",
        "# --- Embeddingleri kaydet ---\n",
        "embeddings_path = \"/content/drive/MyDrive/EnerjiChatbot/embeddings.npy\"\n",
        "np.save(embeddings_path, embeddings)\n",
        "\n",
        "print(f\"Embeddingler kaydedildi: {embeddings_path}\")\n"
      ],
      "metadata": {
        "id": "X3V4qwZ_zLu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# --- Embeddingleri yükle ---\n",
        "embeddings_path = \"/content/drive/MyDrive/EnerjiChatbot/embeddings.npy\"\n",
        "embeddings = np.load(embeddings_path)\n",
        "\n",
        "# --- FAISS index oluştur ---\n",
        "embedding_dim = embeddings.shape[1]  # boyut sayısı\n",
        "index = faiss.IndexFlatL2(embedding_dim)  # CPU üzerinde L2 mesafesi ile\n",
        "\n",
        "# --- Embeddingleri indexe ekle ---\n",
        "index.add(embeddings)\n",
        "print(f\"Toplam vektör sayısı indexte: {index.ntotal}\")\n",
        "\n",
        "# --- Indexi kaydet ---\n",
        "index_path = \"/content/drive/MyDrive/EnerjiChatbot/faiss_index.index\"\n",
        "faiss.write_index(index, index_path)\n",
        "\n",
        "print(f\"FAISS index kaydedildi: {index_path}\")\n"
      ],
      "metadata": {
        "id": "fhWndIohzZfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# --- Embedding modelini yükle ---\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "# --- FAISS indexi yükle ---\n",
        "index_path = \"/content/drive/MyDrive/EnerjiChatbot/faiss_index.index\"\n",
        "index = faiss.read_index(index_path)\n",
        "\n",
        "# --- Chunk verilerini json'dan yükle ---\n",
        "import json\n",
        "with open(\"/content/drive/MyDrive/EnerjiChatbot/enerji_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def query_faiss(user_question, top_k=3):\n",
        "    # Sorguyu embedding'e çevir\n",
        "    query_vec = model.encode([user_question])\n",
        "\n",
        "    # FAISS ile en yakın chunk'ları bul\n",
        "    distances, indices = index.search(query_vec, top_k)\n",
        "\n",
        "    results = []\n",
        "    for dist, idx in zip(distances[0], indices[0]):\n",
        "        results.append({\n",
        "            \"chunk_id\": idx,\n",
        "            \"distance\": float(dist),\n",
        "            \"text\": data[idx][\"text\"]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "soru = \"Buzdolabını doğru kullanarak enerji tasarrufu nasıl yapabilirim?\"\n",
        "sonuclar = query_faiss(soru, top_k=3)\n",
        "\n",
        "for r in sonuclar:\n",
        "    print(f\"[Chunk ID {r['chunk_id']}] (Distance: {r['distance']:.4f})\")\n",
        "    print(r['text'])\n",
        "    print(\"-\"*50)\n"
      ],
      "metadata": {
        "id": "Kb0Dxt2ez-mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,  # CPU için\n",
        "    device_map=\"cpu\"            # CPU kullan\n",
        ")\n"
      ],
      "metadata": {
        "id": "E4BGEZ8T7pl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# JSON CHUNK DATASINI YÜKLE\n",
        "with open(\"/content/drive/MyDrive/EnerjiChatbot/enerji_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "query = \"Buzdolabı enerji tasarrufu için ne yapmalıyım?\"\n",
        "\n",
        "query_vec = embedder.encode([query]).astype(\"float32\")\n"
      ],
      "metadata": {
        "id": "HTAZh72V868f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "\n"
      ],
      "metadata": {
        "id": "jeTu6UA4_lft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.read_index(\"/content/drive/MyDrive/EnerjiChatbot/faiss_index.index\")\n",
        "\n",
        "k = 3\n",
        "scores, ids = index.search(query_vec, k)\n",
        "\n",
        "retrieved_chunks = [data[i][\"text\"] for i in ids[0]]\n"
      ],
      "metadata": {
        "id": "xHAM5AZj8-t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Aşağıdaki metinlere dayanarak kullanıcı sorusunu cevapla.\n",
        "Metinler:\n",
        "{context}\n",
        "\n",
        "Soru: {query}\n",
        "\n",
        "Doğal bir dille cevap ver.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "output = llm.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=300,\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "saH-IeN39ArA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}